{
    "comments": [
        {
            "author": "Aviran Mordo",
            "body": "I wrote a fix that uses the stored values in case the sort field is tokenized\nand stored, or uses the Terms in case the sort field is a Keyword.\n\nIndex: FieldCacheImpl.java\n===================================================================\nRCS file:\n/home/cvspublic/jakarta-lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java,v\nretrieving revision 1.3\ndiff -u -r1.3 FieldCacheImpl.java\n--- FieldCacheImpl.java\t21 Jul 2004 19:05:46 -0000\t1.3\n+++ FieldCacheImpl.java\t28 Jul 2004 17:45:41 -0000\n@@ -25,6 +25,8 @@\n import java.util.Map;\n import java.util.WeakHashMap;\n import java.util.HashMap;\n+import org.apache.lucene.document.Field;\n+import java.util.Arrays;\n \n /**\n  * Expert: The default cache implementation, storing all values in memory. @@\n-80,6 +82,29 @@\n     }\n   }\n \n+  class FieldEntry implements Comparable {\n+    String val;\n+    int ind;\n+    FieldEntry(int ind, String val)\n+    {\n+        this.ind = ind;\n+        this.val = val;\n+    }\n+    public String getVal()\n+    {\n+        return val;\n+    }\n+    public int getInd()\n+    {\n+        return ind;\n+    }\n+    public int compareTo(Object obj)\n+    {\n+        return val.compareToIgnoreCase(((FieldEntry)obj).getVal());\n+    }\n+}\n+\n+\n \n   /** The internal cache. Maps Entry to array of interpreted term values. **/\n   final Map cache = new WeakHashMap();\n@@ -240,54 +265,92 @@\n     if (ret == null) {\n       final int[] retArray = new int[reader.maxDoc()];\n       String[] mterms = new String[reader.maxDoc()+1];\n-      if (retArray.length > 0) {\n-        TermDocs termDocs = reader.termDocs();\n-        TermEnum termEnum = reader.terms (new Term (field, \"\"));\n-        int t = 0;  // current term number\n-\n-        // an entry for documents that have no terms in this field\n-        // should a document with no terms be at top or bottom?\n-        // this puts them at the top - if it is changed, FieldDocSortedHitQueue\n-        // needs to change as well.\n-        mterms[t++] = null;\n \n-        try {\n-          if (termEnum.term() == null) {\n-            throw new RuntimeException (\"no terms in field \" + field);\n-          }\n-          do {\n-            Term term = termEnum.term();\n-            if (term.field() != field) break;\n-\n-            // store term text\n-            // we expect that there is at most one term per document\n-            if (t >= mterms.length) throw new RuntimeException (\"there are more\nterms than documents in field \\\"\" + field + \"\\\"\");\n-            mterms[t] = term.text();\n-\n-            termDocs.seek (termEnum);\n-            while (termDocs.next()) {\n-              retArray[termDocs.doc()] = t;\n-            }\n-\n-            t++;\n-          } while (termEnum.next());\n-        } finally {\n-          termDocs.close();\n-          termEnum.close();\n+      Field docField = reader.document(0).getField(field);\n+      if (docField.isStored() && docField.isTokenized()) {\n+          // Fill entries\n+        FieldEntry[] entries = new FieldEntry[reader.maxDoc()];\n+        for (int i=0; i<reader.maxDoc(); i++) {\n+          String fieldValue;\n+          if (!reader.isDeleted(i))\n+            fieldValue = reader.document(i).get(field);\n+          else\n+            fieldValue = \"\";\n+          entries[i] = new FieldEntry (i,fieldValue);\n         }\n \n-        if (t == 0) {\n-          // if there are no terms, make the term array\n-          // have a single null entry\n-          mterms = new String[1];\n-        } else if (t < mterms.length) {\n-          // if there are less terms than documents,\n-          // trim off the dead array space\n-          String[] terms = new String[t];\n-          System.arraycopy (mterms, 0, terms, 0, t);\n-          mterms = terms;\n+        Arrays.sort(entries);\n+        for (int i=0;i<reader.maxDoc();i++)\n+        {\n+          int ind = entries[i].getInd();\n+          retArray[ind] = i;\n+          mterms[ind]=entries[i].getVal();\n         }\n       }\n+      else\n+      {\n+          if (retArray.length > 0)\n+          {\n+              TermDocs termDocs = reader.termDocs();\n+              TermEnum termEnum = reader.terms(new Term(field, \"\"));\n+              int t = 0; // current term number\n+\n+              // an entry for documents that have no terms in this field\n+              // should a document with no terms be at top or bottom?\n+              // this puts them at the top - if it is changed,\nFieldDocSortedHitQueue\n+              // needs to change as well.\n+              mterms[t++] = null;\n+\n+              try\n+              {\n+                  if (termEnum.term() == null)\n+                  {\n+                      throw new RuntimeException(\"no terms in field \" + field);\n+                  }\n+                  do\n+                  {\n+                      Term term = termEnum.term();\n+                      if (term.field() != field)\n+                          break;\n+\n+                      // store term text\n+                      // we expect that there is at most one term per document\n+                      if (t >= mterms.length)\n+                          throw new RuntimeException(\"there are more terms than\ndocuments in field \\\"\" + field +\n+                                                     \"\\\"\");\n+                      mterms[t] = term.text();\n+                      termDocs.seek(termEnum);\n+                      while (termDocs.next())\n+                      {\n+                          retArray[termDocs.doc()] = t;\n+                      }\n+\n+                      t++;\n+                  }\n+                  while (termEnum.next());\n+              }\n+              finally\n+              {\n+                  termDocs.close();\n+                  termEnum.close();\n+              }\n+\n+              if (t == 0)\n+              {\n+                  // if there are no terms, make the term array\n+                  // have a single null entry\n+                  mterms = new String[1];\n+              }\n+              else if (t < mterms.length)\n+              {\n+                  // if there are less terms than documents,\n+                  // trim off the dead array space\n+                  String[] terms = new String[t];\n+                  System.arraycopy(mterms, 0, terms, 0, t);\n+                  mterms = terms;\n+              }\n+          }\n+      }\n       StringIndex value = new StringIndex (retArray, mterms);\n       store (reader, field, STRING_INDEX, value);\n       return value;\n@@ -309,7 +372,7 @@\n   // inherit javadocs\n   public Object getAuto (IndexReader reader, String field)\n   throws IOException {\n-    field = field.intern();\n+  field = field.intern();\n     Object ret = lookup (reader, field, SortField.AUTO);\n     if (ret == null) {\n       TermEnum enumerator = reader.terms (new Term (field, \"\"));\n",
            "date": "2004-07-29T20:55:50.000+0000",
            "id": 0
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Aviran, could you attach your diff (instead of pasting it in), please?\nUse the 'Create a new attachment' link above.  This will keep the patch nice and\nclean, without any wrapped lines and such.  Thanks!",
            "date": "2004-07-30T16:26:03.000+0000",
            "id": 1
        },
        {
            "author": "Aviran Mordo",
            "body": "Created an attachment (id=12275)\nA diff patch file that uses the stored values in case the sorted field is tokenized\n",
            "date": "2004-07-30T20:03:07.000+0000",
            "id": 2
        },
        {
            "author": "Enis Soztutar",
            "body": "In the project Nutch, we have encountered a subtle bug, which I tracked down and found to be related to unintuitive caching in tokenized fields. \n\nnutch uses several index servers, and the search results from these servers are merged using a dedup field for for deleting dupilcates. The values from this field is cached by FieldCachImpl. The default is the site field, which is indexed and tokenized. However for a Tokenized Field (for example \"url\" in nutch), FieldCacheImpl returns an array of Terms rather that array of field values, so dedup'ing becomes faulty. \n\nCurrent FieldCache implementation does not respect tokenized fields, and as described above caches only terms. I have ported the previous patch and improved it for the 2.0 branch. And i will write a patch for the trunk. \n\nI am voting for this patch to be committed. ",
            "date": "2007-03-06T12:19:42.703+0000",
            "id": 3
        },
        {
            "author": "Enis Soztutar",
            "body": "A bug in FieldCacheImpl_Tokenized_fields_lucene_2.0.patch is fixed, which caused not storing newly built cache. This patch obsoletes FieldCacheImpl_Tokenized_fields_lucene_2.0.patch\n",
            "date": "2007-03-06T13:07:24.671+0000",
            "id": 4
        },
        {
            "author": "Enis Soztutar",
            "body": "This version patches against the current trunk version 2.2-dev. Please see the above comments about the contents of the patch. ",
            "date": "2007-03-06T13:12:38.122+0000",
            "id": 5
        },
        {
            "author": "Doug Cutting",
            "body": "I am not convinced that we should encourage folks to use a FieldCache with a tokenized field in this way, expecting the untokenized, stored value.  It is considerably slower to populate the cache this way, greatly reducing its utility.  Instead we should probably improve the documentation on this point, and perhaps even throw an exception for tokenized fields.",
            "date": "2007-03-07T19:14:15.336+0000",
            "id": 6
        },
        {
            "author": "Hoss Man",
            "body": "FieldCacheImpl.getStringIndex already throws an exception if it detects it's being used on a tokenized field (but i'm not sure if it does a perfect job of detecting that ... it seems to make a naive assumption based on the number of terms) ... i'm guessing that check wasn't in the code when this bug was initially filed.\n\nI agree with Doug, silently using the stored field value if/when a field is tokenized is a bad idea .. there is a seperate patch available for doing this that allows people to be much more explicit about their intentions -- see (LUCENE-769)",
            "date": "2007-03-07T23:00:15.279+0000",
            "id": 7
        },
        {
            "author": "Enis Soztutar",
            "body": "Well, I have spent half a day to find this issue with tokenized field caching, so I absolutely agree on throwing and exception in the getStrings() and getStringIndex() functions of FieldCacheImpl. A snippet would be like : \n\nField docField = getField(reader, field);\n      if (docField != null && docField.isStored() && docField.isTokenized()) {\n           throw new RuntimeException(\"Caching in Tokenized Fields is not allowed\");\n      }\n\nLooking at the timing of cache building tokenized fields are really slow, as Doug mentioned, for a 1.5M real index(from web documents) building the cache on a tokenized field takes 1600 ms on the avarage, but for an untokenized field, it takes 30000 ms on avarage. \n\nIn nutch we have 3 options : 1st is to disallow deleting duplicates on tokenized fields(due to FieldCache), 2nd is to index the tokenized field twice(once tokenized, and once untokenized), 3rd use the above patch and warm the cache initially in the index servers. \n\nI am in favor of the 3rd option and believe that this patch is necessary and it can be included with an explanatory javadoc. \nanother option will be to extend the defalut FieldCacheImpl and allow for tokenized field caching and naming the class similar to LUCENE-769's such as StoredFieldCacheImpl. If that is ok, i can prepare a patch and send it here. \n\n\n\n\n",
            "date": "2007-03-08T08:25:18.633+0000",
            "id": 8
        },
        {
            "author": "Yonik Seeley",
            "body": "I wouldn't want to see sorting on a tokenized field disallowed (Lucene's definition of tokenized is that analysis is done on the field value).  There are tokenized fields that simply do things like manipulate a numer into a text-sortable value, lowercase, or do some other sort of normalization that doesn't produce multiple tokens.",
            "date": "2007-03-08T14:59:56.791+0000",
            "id": 9
        },
        {
            "author": "Hoss Man",
            "body": "definitely in agreement with yonik here, erroring out if \"docField.isTokenized()\" would prevent some perfectly valid use cases ... my point was that hte current test of \"if (t >= mterms.length)\" only triggers an error if htere are more total terms in the field then there are documents in the index ... but there can be plenty of situations where a doc has more then one indexed term, but the total number of indexed terms is less hten the number of documents, a better test would be to check and see if we have already recorded a term for this doc.\n\nI have to say: I'm really not understanding how the current behavior is hindering nutch ... my understanding of the nutch model is that the set of fields is very well known -- why do you need to rely on FieldCache being smart enough to stop you from trying to sort on a tokenized field? (and what does that have to do with deleting duplicates?)\n\nif nothing else: if nutch needs to prevent using FieldCache based sorting on tokenized fields, why can't the \"if (docField.isTokenized())\" logic be done outside of the FieldCacheImpl ... possibly as a way to decide if you want to use the basic sorting or use something like LUCENE-769?\n\n\n...perhaps this is something that should be discussed more on java-dev?\n\n",
            "date": "2007-03-08T16:49:35.357+0000",
            "id": 10
        },
        {
            "author": "Enis Soztutar",
            "body": "I also agree with tokenized field caching, which is a use case for nutch. Let me elaborate on the use case. In a nutch deployment, we generate indexes from the web documents, and indeed the set of fields is known a priori. Then the indexes are distributed to several index servers running on hadoop's RPC calls. Then the query is sent to all of the index servers, the results are collected and merged on the fly. Since the indexes need not be disjoint(since crawling is an adaptive process) the results should be merged, without having a document more then once. So we need a unique key to represent the document. Default nutch codebase uses the site field(url's hostname), which is untokenized for such a task, and allow only 1 - 2 documents from a site in the search results. For obvious performance reasons, the site field is cached in the index servers with FieldCache.getStrings(). The problem arises when we want to show more than one result from a specific site (for example in a site:apache.org query ), and if we have the same url indexed in more than one index server. We use the tokenized url field in the FieldCache, then deleting duplicates becomes error prone. Since we use FieldCache.getStrings() rather that FieldCache.getStringIndex(), the problem here is not tokenized field sorting, but tokenized field not caching correctly, an example of which is an array like [com, edu. www, youtube, ] from the getStrings() method(for each doc, only a token is returned, rather than the whole url). \n\nWell, if you are still with me, here is my proposal : \n\n1. in FieldCacheImpl.java in both getStrings and getStringIndex functions add \n\nField docField = getField(reader, field);\n      if (docField != null && docField.isStored() && docField.isTokenized()) {\n           throw new RuntimeException(\"Caching in Tokenized Fields is not allowed\");\n      } \n\n2. subclass FieldCacheImpl as StoredFieldCacheImpl and implement stored field caching there, delegating untokenized fields to super class\n3. add the implementation to FieldCache.java :\n\n public static FieldCache DEFAULT = new FieldCacheImpl();\n public static FieldCache STORED_CACHE = new StoredCacheImpl();\n\nthis way both lucene internals will not be affected and a stored field caching could be performed. \n\n",
            "date": "2007-03-09T10:27:18.025+0000",
            "id": 11
        },
        {
            "author": "Hoss Man",
            "body": "I'm afraid i'm still not understanding the issue in nutch, it seems like the root of hte problem is..\n\n> ... We use the tokenized url field in the FieldCache ...\n\n...if you know this field is tokenized, don't use it this way.  if you want to use it this way, index it a second time untokenized.\n\nAt a more practical level:\n\n1) the change you propose to getStrings and getStringIndex is not practical because as we've discussed before, a field being tokenized isn't a garuntee that FieldCache won't work -- isTokenized just inidcates that an Analyzer was used -- it doesn't indicate that any real tokenization took place (the analyzer might have just been used to lowercase the field value before indexing, or strip off leading/trailing white space) that doesn't mean the normal FieldCache can't be used for sorting.  the converse is also true: !isTokenized doens't tell you that it's safe to build the FieldCache -- even if no Analyzer is ever used, multiple Field values can be added for the same field -- and that is hte root cause of hte problem, not tokenization but multiple terms for a given field.\n\n2) the desired behavior you are requesting in a StoredFieldCacheImpl could be done without making any changes to what so ever to FieldCacheImpl -- since nutch knows exactly which fields it's indexing multiple tokens for, it can make the choice between using a StoredFieldCacheImple or using a FieldCacheImpl. (but as i've said, i really don't think that's the right solution)",
            "date": "2007-03-09T16:55:17.619+0000",
            "id": 12
        },
        {
            "author": "Enis Soztutar",
            "body": "I should admit that, considering the case Yonik has mentioned. throwing an exception by checking the Field.isTokenized() is not suitable. However the check if (t >= mterms.length) is only in getStringIndex() and not in getStrings(). I think that a more robust check then the aforementioned should be included in both getStrings and getStringIndex functions. A possibility would be to allocate a boolean array(or BitSet) of the same size with the retArray, and  then use the array to avoid multiple terms per document.  \n\n> 2) the desired behavior you are requesting in a StoredFieldCacheImpl could be done without making any changes to what so ever to FieldCacheImpl -- since nutch knows exactly which fields it's indexing multiple tokens for, it can make the choice between using a StoredFieldCacheImple or using a FieldCacheImpl.\n\nfrom my previous post  :  In nutch we have 3 options : 1st is to disallow deleting duplicates on tokenized fields(due to FieldCache), 2nd is to index the tokenized field twice(once tokenized, and once untokenized), 3rd use the above patch and warm the cache initially in the index servers.\n\nYes indexing a field a second time is an option, but considering my use cases with nutch, why would i want to grow my index by indexing the field twice, instead of tolerating 30 seconds of cache building in a web server, which will serve the indexes for days or even weeks. \n\nwith a class like StoredFieldCacheImpl we can get the desired behaviour w/o modifiying the FieldCacheImpl, and my suggestion in my previous post  without the 1st part does just this. I couldl have sent this to nutch but i think it is a lucene issue. \n\nAny more suggestions ?\n\n\n\n",
            "date": "2007-03-13T16:55:19.897+0000",
            "id": 13
        },
        {
            "author": "Mark Miller",
            "body": "This issue is too old - if a new patch/proposal is brought up we can reopen it.",
            "date": "2009-12-08T03:51:44.735+0000",
            "id": 14
        }
    ],
    "component": "core/search",
    "description": "When you set s SortField to a Text field which gets tokenized\nFieldCacheImpl uses the term to do the sort, but then sorting is off \nespecially with more then one word in the field. I think it is much \nmore logical to sort by field's string value if the sort field is Tokenized and\nstored. This way you'll get the CORRECT sort order",
    "hasPatch": false,
    "hasScreenshot": false,
    "id": "LUCENE-252",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "[PATCH] Problem with Sort logic on tokenized fields",
    "systemSpecification": "other",
    "version": "1.4"
}
{
    "comments": [
        {
            "author": "Doug Cutting",
            "body": "Can you please attach diffs rather than complete files?  The diffs should not not contain CHANGE comments.  To generate diffs, check Lucene out of Subversion, make your changes, then, from the Lucene trunk, run something like 'svn diff > my.patch'.  New files should first be added with 'svn add' so that they're included in the diff.  Thanks!\n\n",
            "date": "2006-05-09T09:24:34.000+0000",
            "id": 0
        },
        {
            "author": "Ning Li",
            "body": "Here is the diff file of IndexWriter.java.",
            "date": "2006-05-10T00:52:39.000+0000",
            "id": 1
        },
        {
            "author": "Otis Gospodnetic",
            "body": "I took a look at the patch and it looks good to me (anyone else had a look)?\nUnfortunately, I couldn't get the patch to apply :(\n\n$ patch -F3 < IndexWriter.patch\n(Stripping trailing CRs from patch.)\npatching file IndexWriter.java\nHunk #1 succeeded at 58 with fuzz 1.\nHunk #2 succeeded at 112 (offset 2 lines).\nHunk #4 succeeded at 504 (offset 33 lines).\nHunk #6 succeeded at 605 with fuzz 2 (offset 57 lines).\nmissing header for unified diff at line 259 of patch\n(Stripping trailing CRs from patch.)\ncan't find file to patch at input line 259\nPerhaps you should have used the -p or --strip option?\nThe text leading up to this was:\n...\n...\n...\nFile to patch: IndexWriter.java\npatching file IndexWriter.java\nHunk #1 FAILED at 802.\nHunk #2 succeeded at 745 with fuzz 2 (offset -131 lines).\n1 out of 2 hunks FAILED -- saving rejects to file IndexWriter.java.rej\n\n\nWould it be possible for you to regenerate the patch against IndexWriter in HEAD?\n\nAlso, I noticed ^Ms in the patch, but I can take care of those easily (dos2unix).\n\nFinally, I noticed in 2-3 places that the simple logging via \"infoStream\" variable was removed, for example:\n-    if (infoStream != null) infoStream.print(\"merging segments\");\n\nPerhaps this was just an oversight?\n\nLooking forward to the new patch. Thanks!",
            "date": "2006-07-06T10:25:15.000+0000",
            "id": 2
        },
        {
            "author": "Ning Li",
            "body": "For an overview of my changes, I'll repeat some of what I said in\nmy earlier e-mail (see http://www.gossamer-threads.com/lists/lucene/java-dev/35143),\nthen add more detail about specific coding changes.\n\nOverview\n--------\nToday, applications have to open/close an IndexWriter and\nopen/close an IndexReader directly or indirectly (via IndexModifier)\nin order to handle a mix of inserts and deletes. This performs well\nwhen inserts and deletes come in fairly large batches. However, the\nperformance can degrade dramatically when inserts and deletes are\ninterleaved in small batches. This is because the ramDirectory is\nflushed to disk whenever an IndexWriter is closed, causing a lot of\nsmall segments to be created on disk, which eventually need to be\nmerged.\n\nAPI Changes\n-----------\nWe propose adding a \"deleteDocuments(Term term)\" method to\nIndexWriter. Using this method, inserts and deletes can be\ninterleaved using the same IndexWriter.\n\nCoding Changes\n--------------\nCoding changes are localized to IndexWriter. Internally, the new\ndeleteDocuments() method works by buffering the terms to be deleted.\nDeletes are deferred until the ramDirectory is flushed to disk,\neither because it becomes full or because the IndexWriter is closed.\nUsing Java synchronization, care is taken to ensure that an\ninterleaved sequence of inserts and deletes for the same document\nare properly serialized. \n\nFor simplicity of explanation, let's assume the index resides in a\ndisk-based directory.\n\nChanges to the IndexWriter variables:\n  - segmentInfos used to store the info of all segments (on disk\n    or in ram). Now it only stores the info of segments on disk.\n  - ramSegmentInfos is a new variable which stores the info of just\n    ram segments.\n  - bufferedDeleteTerms is a new variable which buffers delete terms\n    before they are applied.\n  - maxBufferedDeleteTerms is similar to maxBufferedDocs. It controls\n    the max number of delete terms that can be buffered before they\n    must be flushed to disk.\n\nChanges to IndexWriter methods:\n  - addDocument()\n    The info of the new ram segment is added to ramSegmentInfos.\n  - deleteDocuments(), batchDeleteDocuments()\n    The terms are added to bufferedDeleteTerms. bufferedDeleteTerms\n    also records the current number of documents buffered in ram, so\n    the delete terms can be applied to ram segments as well as\n    the segments on disk.\n  - flushRamSegments()\n    Step 1: Apply buffered delete terms to all the segments on disk.\n    Step 2: Merge all the ram segments into one segment on disk.\n    Step 3: Apply buffered delete terms to the new segment appropriately,\n            so that a delete term is only applied to the documents\n            buffered before it, but not to those buffered after it.\n    Step 4: Clean up and commit the change to the index (both the new\n            segment and the .del files if it applies).\n  - maybeMergeSegments()\n    Before, a flush would be triggered only if enough documents were\n    buffered. Now a flush is triggered if enough documents are\n    buffered OR if enough delete terms are buffered.\n",
            "date": "2006-07-07T01:46:45.000+0000",
            "id": 3
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Thanks for all the information about coding changes, that makes it easier to understand the diff.\nIdeally this will become comments in the new diff, which I can commit.\n\nYonik mentioned this in email.  It does sound like a better place for this might be in a higher level class.  IndexWriter would really not be just a writer/appender once delete functionality is added to it, even if it's the IndexReaders behind the scenes doing the work.  So if you are going to be redoing the patch, consider this.\n\nPerhaps IndexModifier methods should be deprecated and it should get a new/your API?\n",
            "date": "2006-07-07T04:31:58.000+0000",
            "id": 4
        },
        {
            "author": "Ning Li",
            "body": "Hi Otis,\n\nI've attached two patch files:\n  - IndexWriter.July09.patch is an updated version of the old patch.\n  - NewIndexModifier.July09.patch makes minimal changes to IndexWriter and puts new functionalities in a new class called NewIndexModifier. I didn't name it IndexModifier because the two are unrelated and I don't want a diff of the two.\n\nAll unit test succeeded except the following one:\n    [junit] Testcase: testIndex(org.apache.lucene.index.TestIndexModifier):\tFAILED\n    [junit] expected:<3> but was:<4>\n    [junit] junit.framework.AssertionFailedError: expected:<3> but was:<4>\n    [junit] \tat org.apache.lucene.index.TestIndexModifier.testIndex(TestIndexModifier.java:67)\n\nHowever, the unit test has a problem, not the patch: IndexWriter's docCount() does not tell the actual number of documents in an index, only IndexReader's numDocs() does. For example, in a similar test below, where 10 documents are added, then 1 deleted, then 2 added, the last call to docCount() returns 12, not 11, with or without the patch.\n\n  public void testIndexSimple() throws IOException {\n    Directory ramDir = new RAMDirectory();\n    IndexModifier i = new IndexModifier(ramDir, new StandardAnalyzer(), true);\n    // add 10 documents initially\n    for (int count = 0; count < 10; count++) {\n       i.addDocument(getDoc());\n    }\n    i.flush();\n    i.optimize();\n    assertEquals(10, i.docCount());\n    i.deleteDocument(0);\n    i.flush();\n    assertEquals(9, i.docCount());\n    i.addDocument(getDoc());\n    i.addDocument(getDoc());\n    i.flush();\n    assertEquals(12, i.docCount());\n  }\n\nThe reason for the docCount() difference in the unit test (which does not affect the correctness of the patch) is that flushRamSegments() in the patch merges all and only the segments in ram and write to disk, whereas the original flushRamSegments() merges not only the segments in ram but *sometimes* also one segment from disk (see in that function the comment \"// add one FS segment?\").\n\nRegards,\nNing",
            "date": "2006-07-10T10:38:37.000+0000",
            "id": 5
        },
        {
            "author": "Ning Li",
            "body": "Hopefully, third time's a charm. :-)\n\nI rewrote IndexWriter in such a way that semantically it's the same as before, but it provides extension points so that delete-by-term, delete-by-query, and more functionalities can be easily supported in a subclass. NewIndexModifier is such a subclass that supports delete-by-term.\n\nHere is an overview of the changes:\n\nChanges to IndexWriter\nChanges to IndexWriter variables:\n  - segmentInfos used to store the info of all segments (on disk or in ram). Now it\n    only stores the info of segments on disk.\n  - ramSegmentInfos is a new variable which stores the info of just ram segments.\nChanges to IndexWriter methods:\n  - addDocument()\n    The info of the new ram segment is added to ramSegmentInfos.\n  - maybeMergeSegments()\n    toFlushRamSegments() is called at the beginning to decide whether a flush should take place.\n  - flushRamSegments()\n    doAfterFlushRamSegments() is called after all ram segments are merged and flushed to disk.\n\nNewIndexModifier\nNew variables:\n  - bufferedDeleteTerms is a new variable which buffers delete terms\n    before they are applied.\n  - maxBufferedDeleteTerms is similar to maxBufferedDocs. It controls\n    the max number of delete terms that can be buffered before they\n    must be flushed to disk.\nOverloaded/new methods:\n  - deleteDocuments(), batchDeleteDocuments()\n    The terms are added to bufferedDeleteTerms. bufferedDeleteTerms\n    also records the current number of documents buffered in ram,\n    so the delete terms can be applied to ram segments as well as\n    the segments on disk.\n  - toFlushRamSegments()\n    In IndexWriter, a flush would be triggered only if enough documents were\n    buffered. Now a flush is triggered if enough documents are\n    buffered OR if enough delete terms are buffered.\n  - doAfterlushRamSegments()\n    Step 1: Apply buffered delete terms to all the segments on disk.\n    Step 2: Apply buffered delete terms to the new segment appropriately,\n            so that a delete term is only applied to the documents\n            buffered before it, but not to those buffered after it.\n    Step 3: Clean up the buffered delete terms.",
            "date": "2006-07-19T00:32:20.000+0000",
            "id": 6
        },
        {
            "author": "Doron Cohen",
            "body": "I tried out this patch (July18), and have a few comments...\n\nFirst, it is nice to be able to add/remove documents with no need to care for switching between readers and writers, and without worrying for performance issues as result of that switching. I did not test for performance yet.\n\nThis post is quite long, so here is an outline...\n(1) Compile error in test code\n(2) Failing tests - is this patch actually fixing a bug in current flushRamSegments()?\n(3) Additional tests I ran\n(4) Javadocs remarks\n(5) deleteDocument(int doc) not implemented\n(6) flush() not implemented\n(7) Method name - batchDeleteDocuments(Term[]) \n(8) Class name and placement + What's Next for this patch\n\n------------ (1) Compile error in test code \nThe new TestWriterDelete does not reflect recent name change from IndexWriter to NewIndexModifier. Easily fixed by renaming accordingly in that file.\n\n------------ (2) Failing tests - does this patch also fix a bug in current flushRamSegments()?\n\"ant test\" has one failure: TestIndexModifier.testIndex().\nThis is the same issue that Ning described above. However I think it exposes a bug in current flushRamSegments(): when an index with 1 segment on disk that has 2 documents, one of which is deleted, and 1 segment in memory, is closed, this method decides to merge - prematurely - the two segments into one. This wrong behavior (if I understand things correctly) is - by \"mistake\" - causing TestIndexModifier.testIndex() to pass in the current implementation of flushRamSegments(). But this comes with the cost of too many merges. If one is interleaving adds and deletes this bug would become costly. I will post a separate question on this to the dev forum, to discuss if this is indeed a bug.\n\n------------ (3) Additional tests I ran \nI wanted to verify that all existing functions (well, at least tested ones..) are working with the new class (NewIndexModifier). So I temporarily renamed the existing IndexWriter to IndexWriter0, and renamed NewIndexModifier to IndexWriter (now extending IndexWriter0). For compiling, now, also had to temporarily modify args from IndexWriter to IndexWriter0 in 3 classes - DocumentWriter, SegmentMerger, and also from NewIndexModifier to IndexWriter in the new TestWriteDelete. (Note again: these modifications are temporary, just for the sake of testing this new class as if it was the new IndexWriter, which it is not.) Now all the tests were using the new class instead of the original IndexWriter. \n\nAll tests passed, except for TestIndexModifier.testIndex() - this is the same failure as above - so, no problem detected in new class.\n\n------------ (4) Javadocs Remarks\nCurrent Javadocs for the new class focus on changes to the implementation. I think this description of implementation changes should be made regular Java comments (for developers), and instead should add a shorter javadoc that describes the API for users, and the implications on behavior as result of buffering deletes.\n\n------------ (5) deleteDocument(int doc) not implemented\nOriginal IndexModifier has a delete(int docs), the new class doesn't. At first this seems ok, since internal doc IDs are not accessible through index writer (unlike index reader). But IndexModifier also does not provide access to doc-ids. So why was delete-by-id enabled in IndexModifier? Perhaps there's a good reason for it, that I fail to see - if so, it should probably be added to the new class as well. Adding this is required if the new class would eventually replace the implementation of current index modifier.\n\n------------ (6) flush() not implemented\nOriginal IndexModifier has a flush(int docs) method, allowing to commit any pending changes. I think it would be nice to have this feature here as well, for forcing any pending changes (without caller having to modify the max-bufferred value). This would allow more control when using this class. Again, adding this is required if the new class would eventually replace the implementation of current index modifier.\n\n------------ (7) Method name - batchDeleteDocuments(Term[]) \nI would prefer it to be called deleteDocuments(Term[]), and let Java decide which method to call. Main reason is developers would expect that methods with similar semantics are named similarly, especially when using IDEs like Eclipse, where users type \"i.del\" and the IDE lets them select from all the methods that start with \"del\".\n\n------------ (8) Class name and placement + What's Next for this patch\nPerformance test should be added for this new class. Also, I did not code review the actual code changes to IndexWriter and the code of NewIndexModifier itself.\n\nIt seems to me that this class would be very useful for users, either as a new class or if it replaces the current implementation of IndexModifier. Latter would be possible only if the 2 missing methods mentioned above are added. In this case, the \"immediate delete\" behavior of current IndexModifier should be possible to achieve by users, by setting maxBefferedDeleteTerms to 1.\n\nOne disadvantage of this class vs. current IndexModifier is the ability to add access to further methods of IndexReader. With current IndexModifier this is very simple (though not always efficient) - just follow code template in existing methods, i.e. close writer/reader and open reader/writer as required. With the new class, exposing further methods of IndexReader would be more of a challenge. Perhaps having a multiReader on all segment readers can do. I am not sure to what extent this should be a consideration, so just bringing it up. \n\nSo, If this class replaces IndexModifier - fine. If not, how about calling it BufferredIndexWriter?\n\n- Doron",
            "date": "2006-08-15T04:19:35.000+0000",
            "id": 7
        },
        {
            "author": "Jason Rutherglen",
            "body": "I tested just the IndexWriter from this code base, it does not seem to work.  NewIndexModifier does work.  I simply used IndexWriter to create several documents and then search for them.  Nothing came back even though it seems something was written to disk.",
            "date": "2006-08-22T19:00:37.000+0000",
            "id": 8
        },
        {
            "author": "Ning Li",
            "body": "> Yes I am including this patch as it is very useful for increasing\n> the efficiency of updates as you described.  I will be conducting\n> more tests and will post any results.  Yes a patch for IndexWriter\n> will be useful so that the entirety of this build will work.\n> Thanks!\n\nI've attached a patch that works with the current code. The\nimplementation of IndexWriter and NewIndexModifier is the same as\nthe last patch. I removed the \"singleDocSegmentsCount\" optimization\nfrom this patch since my IndexWriter checks singleDocSegmentsCount\nby simply calling ramSegmentInfos.size().\n\nThis patch had evolved with the help of many good discussions\n(thanks!) since it came out in May. Here is the current state of\nthe patch:\n  - This patch aims at enabling users to do inserts and general\n    deletes (delete-by-term, and later delete-by-query) without\n    switching between writers and readers.\n  - The goal is achieved by rewritting IndexWriter in such a way\n    that semantically it's the same as before, but it provides\n    extension points so that delete-by-term, delete-by-query, and\n    more functionalities can be easily supported in a subclass.\n  - NewIndexModifier extends IndexWriter and supports delete-by-term\n    by simply overriding two methods: toFlushRamSegment() which\n    decides if a flush should happen, and doAfterFlushRamSegments()\n    which does proper work after a flush is done.\n\nSuggestions are welcome! Especially those that may help it get\ncommitted. :-)",
            "date": "2006-08-23T22:42:08.000+0000",
            "id": 9
        },
        {
            "author": "Ning Li",
            "body": "Doron, thank you very much for the review! I want to briefly comment\non one of your comments:\n\n> (5) deleteDocument(int doc) not implemented\n\nI deliberately left that one out. This is because document ids are\nchanging as documents are deleted and segments are merged. Users\ndon't know exactly when segments are merged thus ids are changed\nwhen using IndexModifier. Thus I don't think it should be supported\nin IndexModifier at all.\n",
            "date": "2006-08-23T22:43:58.000+0000",
            "id": 10
        },
        {
            "author": "Jason Rutherglen",
            "body": "This IndexWriter seems to work.  Thanks.  Great work!",
            "date": "2006-08-24T23:42:37.000+0000",
            "id": 11
        },
        {
            "author": "Doron Cohen",
            "body": "I ran a performance test for interleaved adds and removes - and compared between IndexModifier and NewIndexModifier. \n\nFew setups were tested, with a few combinations of \"consecutive adds before a delete takes place\", maxBufferredDocs, and \"number of total test iterations\", where each iteration does the conseutive adds and then does the deletes.\n\nEach setup ran in this order - orig indexModifier, new one, orig, new one, and the best time out of the two runs was used.\n\nResults indicate that NewIndexModifier is far faster for most setups. \n\nAttached is the performance test, the performance results, and the log of the run. The performance test is written as a Junit test, and it fails in case the original IndexModfier is faster than the new one by more than 1 second (smaller than 1 sec difference is considered noise). \n\nTest was run on XP (SP1) with IBM JDK 1.5.\n\nTest was first failing with \"access denied\" errors due to what seems to be an XP issue. So in order to run this test on XP (and probably other Windows platforms) the patch from http://issues.apache.org/jira/browse/LUCENE-665 should be applied first.\n\nIt is interesting to notice that in addition to preformance gain, NewIndexModifier seems less sensitive to \"access denied\" XP problems, because it closes/reopens readers and writers less frequently, and indeed, at least in my runs, these errors had to be bypassed (by the \"retry\" patch) only for the current index-modifier. \n\n- Doron\n\n",
            "date": "2006-08-26T01:21:29.000+0000",
            "id": 12
        },
        {
            "author": "Jason Rutherglen",
            "body": "It seems this writer works, but then some mysterious happens to the index and the searcher can no longer read it.  I am using this in conjunction with Solr.  The index files look ok, however a search will return nothing.  I have seen this repeatedly over about 1 weeks time.",
            "date": "2006-08-29T19:55:25.000+0000",
            "id": 13
        },
        {
            "author": "Doron Cohen",
            "body": "Is it that results that were returned are suddenly (say after updates) not returned anymore (indicating something bad happened to existing index)?\n\nOr is it that the search does not reflect recent changes? \n\nI don't remember how often Solr closes and re-opens the writer/modifier...  with this patch a delete does not immediately cause a \"flush to disk\" - so flushes are controlled by closing the NewIndexModifier (and re-opening, since there no flush() method) and by the limits for max-bufferred-docs and max-bufferred-deletes. If this seems relevant to your case, what limits are in effect?",
            "date": "2006-08-29T20:20:37.000+0000",
            "id": 14
        },
        {
            "author": "Jason Rutherglen",
            "body": "I started to flush the deletes after making them, which opens a new NewIndexModifier afterwards.  I still see the same thing.  I am starting off by deleting all documents by matching on a Term that all of them have.  Commit (reopen), then perform a batch addDocuments.  Then when a search is executed nothing is returned, and after an optimize the index goes down to 1K.  Seems like some peculiarity in NewIndexModifier.  Seems like the new documents are deleted even after they are added.  ",
            "date": "2006-08-29T22:48:22.000+0000",
            "id": 15
        },
        {
            "author": "Doron Cohen",
            "body": "Just to make sure on the scenario - are you - \n(1) using NewIndexModifier at all, or \n(2) just letting Solr use this IndexWriter (with the code changes introduced to ebable NewIndexModifier) instead of the Lucene's svn-head (or cetrain release) IndexModifier. \n\nAs is, Solr would not use NewIndexModifier or IndexModifier at all. \n\nFor case (2) above the bufferred deletes logic is not in effect at all. \n \nI wonder if it possibe to re-create this with a simple Lucene stand-alone (test) program rather than with Solr - it would be easier to analyze.",
            "date": "2006-08-30T00:16:21.000+0000",
            "id": 16
        },
        {
            "author": "Jason Rutherglen",
            "body": "Good points... I've actually used both NewIndexModifier and the parent.  I've tried writing a new UpdateHandler, and incorporating the new IndexWriter into DirectUpdateHandler2.  I will create a non-Solr reproduction of the issue.  I guess it has something to do with ths doc ids being reused and so the new documents that are added are also marked as deleted as the number of documents would match almost exactly after the rebuild.  I am not an expert in regards to that aspect of Lucene.",
            "date": "2006-08-30T00:19:55.000+0000",
            "id": 17
        },
        {
            "author": "Jason Rutherglen",
            "body": "Having trouble reproducing this.  Probably something in the other code.  Thanks for the help and the patch, I feel more confident in it now.  ",
            "date": "2006-08-30T05:40:27.000+0000",
            "id": 18
        },
        {
            "author": "Jason Rutherglen",
            "body": "I figured out the problem, the Solr DirectUpdateHandler2 expects to delete only a certain number of documents specifically the oldest first in some cases by using TermDocs and deleting by the doc id.  NewIndexModifier deletes at the level of the SegmentReader however.  Any good way to do this?",
            "date": "2006-08-31T00:24:24.000+0000",
            "id": 19
        },
        {
            "author": "Doron Cohen",
            "body": "Updated performance test results - perf-test-res2.JPG - in avarage, the new code is  *9*  times faster!\n\nWhat have changed? - in previous test I forgot to set max-buffered-deletes. \n\nAfter fixing so, I removed the test cases with max-buffer of 5,000 and up, because they consumed too much memory, and added more practical (I think) cases of 2000 and 3000. \n\nHere is a textual summary of the data in the attached image:\n\nmax buf add/del         10          10       100      1000     2000      3000\niterations                       1          10        100       100      200          300\nadds/iteration             10          10         10          10       10              10\ndels/iteration                 5            5           5            5        5                  5\norig time (sec)           0.13      0.86        9.57      8.88    22.74      44.01\nnew  time (sec)          0.20      0.95       1.74      1.30    2.16          3.08\nImprovement (sec)    -0.07    -0.09      7.83      7.58    20.58      40.94\nImprovement  (%)     -55%     -11%      82%      85%      90%      93%\n\nNote: for the first two cases new code is slower by 11% and 55%, but this is a very short test case, - the absolute difference here is less than 100ms, comparing to the other cases, where the difference is measured in seconds and 10's of seconds.",
            "date": "2006-09-01T05:11:43.000+0000",
            "id": 20
        },
        {
            "author": "Yonik Seeley",
            "body": "> the new code is *9* times faster! \n\nThat's a bit apples-and-oranges :-)  I don't think people use IndexModifier when they need performance... they buffer things and do them in batches.\n\nIMO, performance of *existing* code using IndexWriter is more important, and what I would be interested in.  Say indexing 10000 documents to a RamDirectory with default settings (no deletes at all).  I haven't had a chance to review the new code, so I don't know if it's something to worry about or not.\n\n",
            "date": "2006-09-01T14:57:55.000+0000",
            "id": 21
        },
        {
            "author": "Yonik Seeley",
            "body": "I believe this patch probably also changes the merge behavior.\nI think we need to discuss what exactly the new merge behavior is, if it's OK, what we think the index invariants should be (no more than x segments of y size, etc), and I'd like to see some code to test those invariants.\n\nKeep in mind the difficulty of getting the last IndexWriter patch correct (and that was a very minor patch to keep track of the number of buffered docs!)",
            "date": "2006-09-01T15:34:11.000+0000",
            "id": 22
        },
        {
            "author": "Doron Cohen",
            "body": "I agree - I also suspected it might change the merge behavior (and also had reflections from the repeated trials to have that simple Indexwriter buffered-docs patch correct...:-). \n\nGuess I just wanted to get a feeling if there is interest to include this patch before I delve into it too much - and the perf test was meant to see for my self if it really helps. I was a bit surprised that it speeds 9 times in an interleaving add/delete scenario. Guess this by itself now justifies delving into this patch, analyzing merge behavior as you suggest - will do - I think idealy should try this patch not to modify the merge behavior.\n\nAbout the test - l was trying to test what I thought is a realistic use scenario (max-buf, etc.) - I have a fixed version of the perf test that is easier to modify for different scenarios - can upload it here if there is interest.\n\n",
            "date": "2006-09-01T19:49:21.000+0000",
            "id": 23
        },
        {
            "author": "Ning Li",
            "body": "This patch features the new more robust merge policy. Reference on the new policy is at http://www.gossamer-threads.com/lists/lucene/java-dev/35147\n  - The patch passes all the tests except that one in TestIndexModifier (see an earlier comment on this issue).\n  - Since the test itself has a problem, it is fixed (one line change) and the patch passes the fixed test.\n  - A new test call TestIndexWriterMergePolicy is included which shows the robustness of the new merge policy.\n\n\nThe following is a detailed description of the new merge policy and its properties.\n\n Overview of merge policy:\n\n A flush is triggered either by close() or by the number of ram segments\n reaching maxBufferedDocs. After a disk segment is created by the flush,\n further merges may be triggered.\n\n LowerBound and upperBound set the limits on the doc count of a segment\n which may be merged. Initially, lowerBound is set to 0 and upperBound\n to maxBufferedDocs. Starting from the rightmost* segment whose doc count\n > lowerBound and <= upperBound, count the number of consecutive segments\n whose doc count <= upperBound.\n\n Case 1: number of worthy segments < mergeFactor, no merge, done.\n Case 2: number of worthy segments == mergeFactor, merge these segments.\n         If the doc count of the merged segment <= upperBound, done.\n         Otherwise, set lowerBound to upperBound, and multiply upperBound\n         by mergeFactor, go through the process again.\n Case 3: number of worthy segments > mergeFactor (in the case mergeFactor\n         M changes), merge the leftmost* M segments. If the doc count of\n         the merged segment <= upperBound, consider the merged segment for\n         further merges on this same level. Merge the now leftmost* M\n         segments, and so on, until number of worthy segments < mergeFactor.\n         If the doc count of all the merged segments <= upperBound, done.\n         Otherwise, set lowerBound to upperBound, and multiply upperBound\n         by mergeFactor, go through the process again.\n Note that case 2 can be considerd as a special case of case 3.\n\n This merge policy guarantees two invariants if M does not change and\n segment doc count is not reaching maxMergeDocs:\n B for maxBufferedDocs, f(n) defined as ceil(log_M(ceil(n/B)))\n      1: If i (left*) and i+1 (right*) are two consecutive segments of doc\n         counts x and y, then f(x) >= f(y).\n      2: The number of committed segments on the same level (f(n)) <= M.\n",
            "date": "2006-09-08T17:55:36.000+0000",
            "id": 24
        },
        {
            "author": "Yonik Seeley",
            "body": "Thanks for separating out the new merge policy Ning!  I'm reviewing the patch now...\nAssuming everything looks good (it does so far), I'm inclined to commit it.  I'm just giving a heads up to other lucene developers as this is a change in behavior to core lucene.\n\nI think the new merge policy is a positive change because:\n - flushing all ram segments separately from disk segments allows more efficient implementations of combination reader/writers (like buffered deletes) because docids won't change from the flush alone (a merge is needed to change ids)\n- flushing all buffered docs together leaves more optimization possibilities... something other than single-doc segments could be used to buffer in-mem docs in the future.\n- increases indexing performance in the presence of deleted documents or partially full segments (merges are minimized while the number of segments are maximized).\n - fixes worst-case behavior that can cause the number of segments to grow too large (way more than mergefactor)\n\nAre there any concerns?",
            "date": "2006-09-12T02:27:45.000+0000",
            "id": 25
        },
        {
            "author": "Yonik Seeley",
            "body": "I also did a quick indexing performance test w/ Solr:\n\nmaxBufferedDocs=100, mergeFactor=4, did 100K random overwriting adds in batches of 75 (75 docs added, dups deleted).\nIt was 12% faster with this new merge policy.",
            "date": "2006-09-14T17:50:06.000+0000",
            "id": 26
        },
        {
            "author": "Ning Li",
            "body": "This is to update the delete-support patch after the commit of the new merge policy.\n  - Very few changes to IndexWriter.\n  - The patch passes all tests.\n  - A new test call TestNewIndexModifierDelete is added to show different scenarios when using delete methods in NewIndexModifier.",
            "date": "2006-09-21T17:15:15.000+0000",
            "id": 27
        },
        {
            "author": "Ning Li",
            "body": "\n   [[ Old comment, sent by email on Thu, 6 Jul 2006 07:53:35 -0700 ]]\n\nHi Otis,\n\nI will regenerate the patch and add more comments. :-)\n\nRegards,\nNing\n\n\n\n\n                                                                           \n             \"Otis Gospodnetic                                             \n             (JIRA)\"                                                       \n             <jira@apache.org>                                          To \n                                       ningli@almaden.ibm.com              \n             07/05/2006 11:25                                           cc \n             PM                                                            \n                                                                   Subject \n                                       [jira] Commented: (LUCENE-565)      \n                                       Supporting deleteDocuments in       \n                                       IndexWriter (Code and Performance   \n                                       Results Provided)                   \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n\n\n\n\n    [\nhttp://issues.apache.org/jira/browse/LUCENE-565?page=comments#action_12419396\n ]\n\nOtis Gospodnetic commented on LUCENE-565:\n-----------------------------------------\n\nI took a look at the patch and it looks good to me (anyone else had a\nlook)?\nUnfortunately, I couldn't get the patch to apply :(\n\n$ patch -F3 < IndexWriter.patch\n(Stripping trailing CRs from patch.)\npatching file IndexWriter.java\nHunk #1 succeeded at 58 with fuzz 1.\nHunk #2 succeeded at 112 (offset 2 lines).\nHunk #4 succeeded at 504 (offset 33 lines).\nHunk #6 succeeded at 605 with fuzz 2 (offset 57 lines).\nmissing header for unified diff at line 259 of patch\n(Stripping trailing CRs from patch.)\ncan't find file to patch at input line 259\nPerhaps you should have used the -p or --strip option?\nThe text leading up to this was:\n...\n...\n...\nFile to patch: IndexWriter.java\npatching file IndexWriter.java\nHunk #1 FAILED at 802.\nHunk #2 succeeded at 745 with fuzz 2 (offset -131 lines).\n1 out of 2 hunks FAILED -- saving rejects to file IndexWriter.java.rej\n\n\nWould it be possible for you to regenerate the patch against IndexWriter in\nHEAD?\n\nAlso, I noticed ^Ms in the patch, but I can take care of those easily\n(dos2unix).\n\nFinally, I noticed in 2-3 places that the simple logging via \"infoStream\"\nvariable was removed, for example:\n-    if (infoStream != null) infoStream.print(\"merging segments\");\n\nPerhaps this was just an oversight?\n\nLooking forward to the new patch. Thanks!\n\nProvided)\n---------------------------------------------------------------------------------\n\n\na\nIndexWriter\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/23049?search_string=indexwriter%20delete;#23049\n\nto\ndeleting\nversion\nusing\nbatches.\n\n--\nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n",
            "date": "2006-11-07T06:22:40.000+0000",
            "id": 28
        },
        {
            "author": "Ning Li",
            "body": "With the recent commits to IndexWriter, this patch no longer applies cleanly. The 5 votes for this issue encourages\nme to submit yet another patch. :-) But before I do that, I'd like to briefly describe the design again and welcome all\nsuggestions that help improve it and help get it committed. :-)\n\nWith the new merge policy committed, the change to IndexWriter is minimal: three zero-or-one-line functions are\nadded and used.\n  1 timeToFlushRam(): return true if number of ram segments >= maxBufferedDocs and used in maybeFlushRamSegments()\n  2 anythingToFlushRam(): return true if number of ram segments > 0 and used in flushRamSegments()\n  3 doAfterFlushRamSegments(): do nothing and called in mergeSegments() if the merge is on ram segments\n\nThe new IndexModifier is a subclass of IndexWriter and only overwrites the three functions described above.\n  1 timeToFlushRam(): return true if number of ram segments >= maxBufferedDocs OR if number of buffered\n     deletes >= maxBufferedDeletes\n  2 anythingToFlushRam(): return true if number of ram segments > 0 OR if number of buffered deletes > 0\n  3 doAfterFlushRamSegments(): properly flush buffered deletes\n\nThe new IndexModifier supports all APIs from the current IndexModifier except one: deleteDocument(int doc).\nI had commented on this before:  \"I deliberately left that one out. This is because document ids are changing\nas documents are deleted and segments are merged. Users don't know exactly when segments are merged\nthus ids are changed when using IndexModifier.\"\n\nThis behaviour is true for both the new IndexModifier and the current IndexModifier. If this is preventing this\npatch from getting accepted, I'm willing to add this, but I will detail this in the Java doc so users of this function\nare aware of this behaviour.\n",
            "date": "2006-11-22T20:28:50.000+0000",
            "id": 29
        },
        {
            "author": "Michael Busch",
            "body": "What are the reasons to not add the NewIndexModifier to Lucene? This issue has already 6 votes, so it seems to be very popular amongst users (there is only one issue that has more votes). I can say that I'm using it for a couple of months already, it works flawlessly and made my life a lot easier ;-) \n\nI think the main objections were that too many changes to IndexWriter were made in the earliest versions of this patch, but with the new merge policy committed, most of the new code is in the new class NewIndexModifier whereas the changes to IndexWriter are minimal. \n\nSo I would like to encourage committer(s) to take another look, I think this would be a nice feature for the next Lucene release.",
            "date": "2006-12-08T15:50:04.000+0000",
            "id": 30
        },
        {
            "author": "Yonik Seeley",
            "body": "Lack of committer time... I've been busy enough that I've shied away from complexity and gravitated toward issues that I can handle in a single bite.  I'm on PTO until the end of the year, so I expect my time to be more compressed.\n\nTo minimize the number of reader open/closes on large persistent segments, I think the ability to apply deletes only before a merge is important.  That might add a 4th method: doBeforeMerge()\n\nIt would be nice to not have to continually open and close readers on segments that aren't involved in a merge.  Is there a way to do this?\n\nNing, please do produce another patch to the latest trunk (but you might want to wait until after LUCENE-702 is sorted out.",
            "date": "2006-12-12T05:20:41.000+0000",
            "id": 31
        },
        {
            "author": "Yonik Seeley",
            "body": "> It would be nice to not have to continually open and close readers on segments\n>  that aren't involved in a merge. Is there a way to do this? \n\nHmmm, and what about segments that are involved in a merge?\nI assume it's a different reader that is used for deleting docs than used for merging, but it doesn't have to be...\n\nIf SegmentInfos had a cached reader, that seems like it would solve both problems.\nI haven't thought about it enough to figure out how doable it is though.\n",
            "date": "2006-12-12T15:10:01.000+0000",
            "id": 32
        },
        {
            "author": "Michael McCandless",
            "body": "> If SegmentInfos had a cached reader, that seems like it would solve both problems.\n> I haven't thought about it enough to figure out how doable it is though.\n\nGood idea!  I think this could also be used by reopen (LUCENE-743 ) to re-use readers.",
            "date": "2006-12-12T15:17:53.000+0000",
            "id": 33
        },
        {
            "author": "Yonik Seeley",
            "body": "> Good idea! I think this could also be used by reopen (LUCENE-743 ) to re-use readers.\n\nYes, although  reopen() needs more support than what would be needed for this though (namely reference counting).\nOne thing to probably watch out for is to avoid making the single-doc ram segments more expensive.",
            "date": "2006-12-12T16:44:55.000+0000",
            "id": 34
        },
        {
            "author": "Yonik Seeley",
            "body": "On 12/12/06, Ning Li <ning.li.li@gmail.com> wrote:\n> > To minimize the number of reader open/closes on large persistent segments, I think the ability to apply deletes only before a merge is important.  That might add a 4th method: doBeforeMerge()\n> \n> I'm not sure I get this. Buffered deletes are only applied(flushed)\n> during ram flush. No buffered deletes are applied in the merges of\n> on-disk segments.\n\nWhat is important is to be able to apply deletes before any ids change.\nYou could do it after every new lowest-level segment is written to the index (the flush), *or* you could choose to do it before a merge of the lowest level on-disk segments.  If none of the lowest level segments have deletes, you could even defer the deletes until after all the lowest-level segments have been merged.  This makes the deletes more efficient since it goes from O(mergeFactor * log(maxBufferedDocs)) to O(log(mergeFactor*maxBufferedDocs))\n\nIf we can't reuse IndexReaders, this becomes more important.\n\nOne could perhaps choose to defer deletes until a segment with deleted docs is involved in a merge.\n \n> > It would be nice to not have to continually open and close readers on segments that aren't involved in a merge.  Is there a way to do this?\n> > If SegmentInfos had a cached reader, that seems like it would solve both problems.\n> > I haven't thought about it enough to figure out how doable it is though.\n> \n> This is a good idea! One concern, however, is that caching readers\n> will cause a larger memory footprint. Is it acceptable?\n\nAs I said, I haven't had time to think about it at all, but at the lowest level of reuse, it wouldn't increase the footprint at all in the event that deletes are deferred until a merge:\n\nThe specific scenario I'm thinking of is instead of\n  doAfterFlushRamSegments()\n    open readers\n    delete docs\n    close readers\n  segmentMerger()\n    open readers\n    merge segments\n    close readers\n\nIt would be:\n  doAfterFlushRamSegments()\n    open readers\n    delete docs\n  segmentMerger()\n    merge segments\n    close readers\n\nThis cutting out an additional open-close cycle.\nYou are right that other forms of reader caching could increase the footprint, but it's nice to have the option of trading some memory for performance.\n\nYet another strategy a subclass of IndexWriter could choose is to only apply deletes to segments actually involved in a merge.  Then the bigger segments in the index wouldn't continually have an reader opened and closed on them.... it could all be deferred until a close, or until there are too many deletes buffered.\n\nOf course NewIndexModifier doesn't have to impliment all these options to start with, but it would be nice if the extension hooks in IndexWriter could support them.\n\nWhew, this is why I was slow to get involved in this again :-)",
            "date": "2006-12-12T18:36:02.000+0000",
            "id": 35
        },
        {
            "author": "Ning Li",
            "body": "> *or* you could choose to do it before a merge of the lowest level on-disk\n> segments.  If none of the lowest level segments have deletes, you could\n> even defer the deletes until after all the lowest-level segments have been\n> merged.  This makes the deletes more efficient since it goes from\n> O(mergeFactor * log(maxBufferedDocs)) to O(log(mergeFactor*maxBufferedDocs))\n\nI don't think I like this semantics, though. With the semantics in the patch,\nan update can be easily supported. With this semantics, an insert is flushed\nyet a delete before the insert may or may not have been flushed.\n\n> You are right that other forms of reader caching could increase the footprint,\n> but it's nice to have the option of trading some memory for performance.\n\nAgree. It'd be nice to cache all readers... :-)\n\nThanks again for your comments. Enjoy your PTO!",
            "date": "2006-12-12T20:30:55.000+0000",
            "id": 36
        },
        {
            "author": "Yonik Seeley",
            "body": "Hmmm, I see your point... If deletes are deferred, a different reader could go and open the index and see the additions but not the deletions.\n\nCan the same thing happen with your patch (with a smaller window), or are deletes applied between writing the new segment and writing the new segments file that references it?  (hard to tell from current diff in isolation)\n\nAnyway, it's less of a problem if opening a new reader is coordinated with the writing.  That still does leave the crash scenario though.\n",
            "date": "2006-12-12T21:29:38.000+0000",
            "id": 37
        },
        {
            "author": "Ning Li",
            "body": "> Can the same thing happen with your patch (with a smaller window), or are deletes applied between writing the new segment and writing the new segments file that references it?  (hard to tell from current diff in isolation)\n\nNo, it does not happen with the patch, no matter what the window size is.\nThis is because results of flushing ram - both inserts and deletes - are committed in the same transaction.",
            "date": "2006-12-13T15:34:00.000+0000",
            "id": 38
        },
        {
            "author": "Yonik Seeley",
            "body": "> both inserts and deletes - are committed in the same transaction.\n\nOK, cool.  I agree that's the ideal default behavior.",
            "date": "2006-12-13T16:01:22.000+0000",
            "id": 39
        },
        {
            "author": "Yonik Seeley",
            "body": "Minor question... in the places that you use Vector, is there a reason you aren't using ArrayList?\nAnd in methods that pass a Vector, that could be changed to a List .",
            "date": "2006-12-13T16:07:15.000+0000",
            "id": 40
        },
        {
            "author": "Ning Li",
            "body": "> Minor question... in the places that you use Vector, is there a reason you aren't using ArrayList? \n> And in methods that pass a Vector, that could be changed to a List . \n\nArrayList and List can be used, respectively.",
            "date": "2006-12-13T17:34:49.000+0000",
            "id": 41
        },
        {
            "author": "Paul Elschot",
            "body": "I'd like to give this a try over the upcoming holidays.\nWould it be possible to post a single patch?\nA single patch can be made by locally svn add'ing all new files\nand then doing an svn diff on all files involved from the top directory.\n\nRegards,\nPaul Elschot\n",
            "date": "2006-12-18T22:18:48.000+0000",
            "id": 42
        },
        {
            "author": "Ning Li",
            "body": "Many versions of the patch were submitted as new code was committed to IndexWriter.java. For each version, all changes made were included in a single patch file.\n\nI removed all but the latest version of the patch. Even this one is outdated by the commit of LUCENE-701 (lock-less commits). I was waiting for the commit of LUCENE-702 before submitting another patch. LUCENE-702 was committed this morning. So I'll submit an up-to-date patch over the holidays.\n\nOn 12/18/06, Paul Elschot (JIRA) <jira@apache.org> wrote:\n> I'd like to give this a try over the upcoming holidays. \n\nThat's great! We can discuss/compare the designs then. Or, we can discuss/compare the designs before submitting new patches.",
            "date": "2006-12-18T22:59:31.000+0000",
            "id": 43
        },
        {
            "author": "Ning Li",
            "body": "Here is the design overview. Minor changes were made because of lock-less commits.\n\nIn the current IndexWriter, newly added documents are buffered in ram in the form of one-doc segments.\nWhen a flush is triggered, all ram documents are merged into a single segment and written to disk.\nFurther merges of disk segments may be triggered.\n\nNewIndexModifier extends IndexWriter and supports document deletion in addition to document addition.\nNewIndexModifier not only buffers newly added documents in ram, but also buffers deletes in ram.\nThe following describes what happens when a flush is triggered:\n\n  1 merge ram documents into one segment and written to disk\n    do not commit - segmentInfos is updated in memory, but not written to disk\n\n  2 for each disk segment to which a delete may apply\n      open reader\n      delete docs*, write new .delN file (* Care is taken to ensure that an interleaved sequence of\n        inserts and deletes for the same document are properly serialized.)\n      close reader, but do not commit - segmentInfos is updated in memory, but not written to disk\n\n  3 commit - write new segments_N to disk\n\nFurther merges for disk segments work the same as before.\n\n\nAs an option, we can cache readers to minimize the number of reader opens/closes. In other words,\nwe can trade memory for better performance. The design would be modified as follows:\n\n  1 same as above\n\n  2 for each disk segment to which a delete may apply\n      open reader and cache it if not already opened/cached\n      delete docs*, write new .delN file\n\n  3 commit - write new segments_N to disk\n\nThe logic for disk segment merge changes accordingly: open reader if not already opened/cached;\nafter a merge is complete, close readers for the segments that have been merged.\n",
            "date": "2006-12-19T00:47:40.000+0000",
            "id": 44
        },
        {
            "author": "Jeremy F. Kassis",
            "body": "Happy New Year everyone. I'm personally very excited about this improvement to Lucene. It really begins to open Lucene up to service highly mutable data, important for the application I'm developing. Following the thread, it looks like quite a few people have favorably reviewed the patch. Perhaps it's time for a blessing and commit? ",
            "date": "2007-01-20T00:33:06.671+0000",
            "id": 45
        },
        {
            "author": "Ning Li",
            "body": "The patch is updated because of the code committed to IndexWriter since the last patch. The high-level design is the same as before. See comments on 18/Dec/06.\n\nCare has been taken to make sure if writer/modifier tries to commit but hits disk full that writer/modifier remains consistent and usable. A test case is added to TestNewIndexModifierDelete to test this.\n\nAll tests pass.",
            "date": "2007-01-26T03:26:49.321+0000",
            "id": 46
        },
        {
            "author": "Michael McCandless",
            "body": "\nThanks for redoing the patch Ning!  I like the added test case for\ndisk full.\n\nI've reviewed this and it looks great.  I fixed a few small typos and\nwhitespace issues (looks like a line-wrapper had jumped in at some\npoint) and attached NewIndexModifier.Jan2007.take2.patch\n\nI think this is the only issue holding up a Lucene 2.1 release (so\nfar?).  Yonik (or anyone) do you have any objections / questions about\nthis patch?  It's basically unchanged from before, just modified to\naccommodate recent fixes to IndexWriter.\n",
            "date": "2007-01-27T12:06:56.962+0000",
            "id": 47
        },
        {
            "author": "Yonik Seeley",
            "body": "I just reviewed this, and it looks good to me.\nI like how you managed to enable parallel analysis in updateDocument() too.\n",
            "date": "2007-01-29T19:03:39.711+0000",
            "id": 48
        },
        {
            "author": "Michael Busch",
            "body": "I tried the new patch out and everything looks good to me. One comment though: The public method NewIndexModifier.flush() just calls the public method flushRamSegments(). It might be confusing to have two public methods that do exactly the same?\n\nBesides this minor question I'm all for committing this patch. ",
            "date": "2007-01-30T13:57:02.020+0000",
            "id": 49
        },
        {
            "author": "Michael McCandless",
            "body": "The flush() was added to better match the current IndexModifier, based\non feedback (bullet 6) above:\n\n    https://issues.apache.org/jira/browse/LUCENE-565#action_12428035\n\nActually, back when that feedback was given, flushRamSegments() was\nstill private.  I agree it's awkward now to have two separate methods\nthat do the same thing.\n\nBut, I prefer \"flush\" over \"flushRamSegments\" because flush() is more\ngeneric so it reveals less about how the IndexWriter makes use of its\nRAM and leaves freedom in the future to have more interesting use of\nRAM (like KinoSearch as one example).\n\nSo I think the right fix would be to add a public IndexWriter.flush()\nthat just calls flushRamSegments, and then make flushRamSegments\nprivate again, then remove the flush() method from NewIndexModifier?\n(The public flushRamSegments() has not yet been released so making it\nprivate again before we release 2.1 is OK).\n\nAny objections to this approach?  I will re-work the last patch &\nattach it.\n",
            "date": "2007-01-30T15:17:38.630+0000",
            "id": 50
        },
        {
            "author": "Michael Busch",
            "body": "Thanks for the explanation, Mike. I'd prefer flush() too and the changes you suggest look good to me!",
            "date": "2007-01-30T16:59:38.754+0000",
            "id": 51
        },
        {
            "author": "Michael McCandless",
            "body": "OK I've attached NewIndexModifier.Jan2007.take3.patch with that approach.\n\nI plan on committing this in the next day or two if there are no more questions/feedback....\n\nThank you Ning for this great addition, and for persisting through this long process!",
            "date": "2007-01-30T17:55:31.683+0000",
            "id": 52
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed this.\n\nThank you Ning.  Keep the patches coming!",
            "date": "2007-02-01T10:58:01.984+0000",
            "id": 53
        },
        {
            "author": "Michael McCandless",
            "body": "Reopening based on recent discussions on java-dev:\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/45099",
            "date": "2007-02-09T22:05:57.313+0000",
            "id": 54
        },
        {
            "author": "Michael McCandless",
            "body": "OK I moved NewIndexModifier's methods into IndexWriter and did some\nsmall refactoring, tightening up protections, fixed javadocs,\nindentation, etc.  NewIndexModifier is now removed.\n\nI like this solution much better!\n\nI also increased the default number of deleted terms before a flush is\ntriggered from 10 to 1000.  These buffered terms use very little\nmemory so I think it makes sense to have a larger default?\n\nSo, this adds these public methods to IndexWriter:\n\n  public void updateDocument(Term term, Document doc, Analyzer analyzer)\n  public void updateDocument(Term term, Document doc)\n  public synchronized void deleteDocuments(Term[] terms)\n  public synchronized void deleteDocuments(Term term)\n  public void setMaxBufferedDeleteTerms(int maxBufferedDeleteTerms)\n  public int getMaxBufferedDeleteTerms()\n\nAnd this public field:\n\n  public final static int DEFAULT_MAX_BUFFERED_DELETE_TERMS = 10;\n\n\nOn the extensions points, we had previously added these 4:\n\n  protected void doAfterFlushRamSegments(boolean flushedRamSegments)\n  protected boolean timeToFlushRam()\n  protected boolean anythingToFlushRam()\n  protected boolean onlyRamDocsToFlush()\n\nI would propose that instead we add only the first one above, but\nrename it to \"doAfterFlush()\".  This is basically a callback that a\nsubclass could use to do its own thing after a flush but before a\ncommit.\n\nBut then I don't think we should add any of the others.  The\n\"timeToFlushRam()\" callback isn't really needed now that we have a\npublic \"flush()\" method.  And the other two are very specific to how\nIndexWriter implements RAM buffering/flushing and so unless/until we\ncan think of a use case that needs these I'm inclined to not include\nthem?\n\nYonik, is there something in Solr that would need these last 2\ncallbacks?\n\nI've attached the patch (LUCENE-565.Feb2007.patch) with these\nchanges!\n",
            "date": "2007-02-10T10:07:44.233+0000",
            "id": 55
        },
        {
            "author": "Yonik Seeley",
            "body": "OK I moved NewIndexModifier's methods into IndexWriter and did some\nsmall refactoring, tightening up protections, \n\n> I would propose that instead we add only the first one above, but rename it to \"doAfterFlush()\". \n\nYes, that sounds fine.\n\nThe problem is that we wouldn't be able to take advantage of the hook because of the \"tightening up protections\".  Access to the segments is key.\n\n+  private SegmentInfos segmentInfos = new SegmentInfos();       // the segments\n+  private SegmentInfos ramSegmentInfos = new SegmentInfos();    // the segments in ramDirectory\n+  final private SegmentInfo buildSingleDocSegment(Document doc, Analyzer analyzer)\n\nSo instead of changing these to private, how about package protected?\n\n",
            "date": "2007-02-13T01:53:24.230+0000",
            "id": 56
        },
        {
            "author": "Michael McCandless",
            "body": "OK, got it.  I will change those 3 to package protection and then commit.  Thanks Yonik.",
            "date": "2007-02-13T10:26:11.395+0000",
            "id": 57
        },
        {
            "author": "Michael McCandless",
            "body": "Closing all issues that were resolved for 2.1.",
            "date": "2007-02-27T18:10:33.369+0000",
            "id": 58
        }
    ],
    "component": "core/index",
    "description": "Today, applications have to open/close an IndexWriter and open/close an\nIndexReader directly or indirectly (via IndexModifier) in order to handle a\nmix of inserts and deletes. This performs well when inserts and deletes\ncome in fairly large batches. However, the performance can degrade\ndramatically when inserts and deletes are interleaved in small batches.\nThis is because the ramDirectory is flushed to disk whenever an IndexWriter\nis closed, causing a lot of small segments to be created on disk, which\neventually need to be merged.\n\nWe would like to propose a small API change to eliminate this problem. We\nare aware that this kind change has come up in discusions before. See\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/23049?search_string=indexwriter%20delete;#23049\n. The difference this time is that we have implemented the change and\ntested its performance, as described below.\n\nAPI Changes\n-----------\nWe propose adding a \"deleteDocuments(Term term)\" method to IndexWriter.\nUsing this method, inserts and deletes can be interleaved using the same\nIndexWriter.\n\nNote that, with this change it would be very easy to add another method to\nIndexWriter for updating documents, allowing applications to avoid a\nseparate delete and insert to update a document.\n\nAlso note that this change can co-exist with the existing APIs for deleting\ndocuments using an IndexReader. But if our proposal is accepted, we think\nthose APIs should probably be deprecated.\n\nCoding Changes\n--------------\nCoding changes are localized to IndexWriter. Internally, the new\ndeleteDocuments() method works by buffering the terms to be deleted.\nDeletes are deferred until the ramDirectory is flushed to disk, either\nbecause it becomes full or because the IndexWriter is closed. Using Java\nsynchronization, care is taken to ensure that an interleaved sequence of\ninserts and deletes for the same document are properly serialized.\n\nWe have attached a modified version of IndexWriter in Release 1.9.1 with\nthese changes. Only a few hundred lines of coding changes are needed. All\nchanges are commented by \"CHANGE\". We have also attached a modified version\nof an example from Chapter 2.2 of Lucene in Action.\n\nPerformance Results\n-------------------\nTo test the performance our proposed changes, we ran some experiments using\nthe TREC WT 10G dataset. The experiments were run on a dual 2.4 Ghz Intel\nXeon server running Linux. The disk storage was configured as RAID0 array\nwith 5 drives. Before indexes were built, the input documents were parsed\nto remove the HTML from them (i.e., only the text was indexed). This was\ndone to minimize the impact of parsing on performance. A simple\nWhitespaceAnalyzer was used during index build.\n\nWe experimented with three workloads:\n  - Insert only. 1.6M documents were inserted and the final\n    index size was 2.3GB.\n  - Insert/delete (big batches). The same documents were\n    inserted, but 25% were deleted. 1000 documents were\n    deleted for every 4000 inserted.\n  - Insert/delete (small batches). In this case, 5 documents\n    were deleted for every 20 inserted.\n\n                                current       current          new\nWorkload                      IndexWriter  IndexModifier   IndexWriter\n-----------------------------------------------------------------------\nInsert only                     116 min       119 min        116 min\nInsert/delete (big batches)       --          135 min        125 min\nInsert/delete (small batches)     --          338 min        134 min\n\nAs the experiments show, with the proposed changes, the performance\nimproved by 60% when inserts and deletes were interleaved in small batches.\n\n\nRegards,\nNing\n\n\nNing Li\nSearch Technologies\nIBM Almaden Research Center\n650 Harry Road\nSan Jose, CA 95120",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-565",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Supporting deleteDocuments in IndexWriter (Code and Performance Results Provided)",
    "systemSpecification": null,
    "version": ""
}
{
    "comments": [
        {
            "author": "Andrzej Bialecki ",
            "body": "Patch relative to the current trunk.",
            "date": "2009-08-15T14:58:42.819+0000",
            "id": 0
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "Updated patch against trunk/ . This patch is a major refactoring that opens way for other implementations of stored fields and postings pruning. Two policies are included in this patch - the original Carmel method, and a simple TF-based threshold method.",
            "date": "2009-11-02T14:36:21.571+0000",
            "id": 1
        },
        {
            "author": "Steve Rowe",
            "body": "Andzrej, when I try to look at [the PDF you posted|http://wiki.apache.org/lucene-java/StaticIndexPruning?action=AttachFile&do=get&target=pruning.pdf] on [the StaticIndexPruning wiki page|http://wiki.apache.org/lucene-java/StaticIndexPruning], Adobe Acrobat gives me the following error:\n\n{quote}\nCannot extract the embedded font 'CAAAA+ArialMT'.  Some characters may not display or print correctly.\n{quote}\n\nand the text is illegible - everything except the page titles looks like a series of dots.",
            "date": "2009-11-06T18:36:18.878+0000",
            "id": 2
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "There have been problems with PDF uploads since the recent Wiki upgrade ... I'll keep trying until it gets through in one piece. Sorry ...",
            "date": "2009-11-06T18:46:22.470+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "Andrzej, i tested your patch. I found two places where @override was on an interface, only problem so far.\n\nhere are some results on the hamshahri persian test collection (I used TF method with -t 2)\n\n||Measure||Unpruned||Pruned||\n|index size|98627KB|42339KB|\n|map|0.4809|0.4241|\n|recip_rank|0.8368|0.8393|\n|P5|0.6277|0.6369|\n|P10|0.5677|0.5785|\n|P15|0.5436|0.5231|\n|P20|0.5185|0.4969|\n|P30|0.4703|0.4385|\n|P100|0.2782|0.2440|\n\nthe queries in this corpus are somewhat general, but seems to be a nice way to reduce the index to more than half its size, still with reasonable quality.\n",
            "date": "2009-11-10T03:46:48.701+0000",
            "id": 4
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "Nice job, Robert - thanks! BTW, your results show an effect that was reported in the papers on this subject, namely that some metrics may actually improve, like MRR and P@10 above.",
            "date": "2009-11-10T09:50:23.310+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "Andrzej, are you still working on the carmel policy? \nI see -conf isn't yet implemented, and I can't seem to get it to prune anything with just a default threshold... guessing its still work in progress?\n",
            "date": "2009-11-10T15:10:53.079+0000",
            "id": 6
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "Default threshold of what? When using the Carmel method, the threshold value should be between 0.0 - 1.0, where 1.0 means no pruning, i.e. 100% of docs are retained. I'm sorry for the confusion - the documentation should be clearer on this point.",
            "date": "2009-11-11T13:38:06.538+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "bq. Default threshold of what?\n\nWhat was confusing me is that the console output always says \"deleted: 0\" for -impl carmel\nFor -impl tf, the console output is correct.\n\nBut looking at the resulting index (which I should have done earlier, sorry), I can see that -impl carmel does work.",
            "date": "2009-11-11T14:45:43.260+0000",
            "id": 8
        },
        {
            "author": "Uwe Schindler",
            "body": "Code seems to be Java 1.5, which is good, but I am wondering about some @SuppressWarnings e.g. in getFieldNames(). The original overriden method returns Collection<String>, if you change that to return the correct type it doesn't need SuppressWarnings. There are more places. Also if you use Collections.<Type>emptyMap() and so on, it is also type safe.\n\nAlso we use no space after comma in Generic type parameters.\n\nBut I like the patch, nice work!",
            "date": "2009-11-11T15:05:37.059+0000",
            "id": 9
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "I'll prepare a new patch - the reason for these deficiencies is that I worked against trunk just before the generics patches were applied ;)",
            "date": "2009-11-11T19:23:02.103+0000",
            "id": 10
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "Updated patch relative to branch_3x.",
            "date": "2010-05-18T15:18:43.400+0000",
            "id": 11
        },
        {
            "author": "Robert Muir",
            "body": "Hi Andrzej, thanks for updating the patch.\n\nI am curious about package organization here, do you anticipate adding some additional pruning functionality in the future that would be different than an index modification tool?\n\nI only ask, because looking at reorganizing our contrib area (LUCENE-2323), I've often thought that perhaps we need a \"contrib/index\" for all the index-related tools, instead of having various ones in \"miscellaneous\", and I wonder what your opinions are on that.\n\nIn any event we could always reorganize this after this issue is resolved if thats the best thing to do, and it could temporarily be contrib/pruning, its just svn moves.\n",
            "date": "2010-05-18T16:41:13.924+0000",
            "id": 12
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "I'm fine with reorganizing it - I originally put this into contrib/pruning to avoid polluting the contrib/misc. If we end up putting this stuff in contrib/index together with other tools then perhaps we should create sub-packages for related functionality, otherwise it would look messy.",
            "date": "2010-05-18T16:52:11.390+0000",
            "id": 13
        },
        {
            "author": "Doron Cohen",
            "body": "Hi Andrzej, Robert, please note that IBM holds a [patent on Lossy index compression|http://patft.uspto.gov/netacgi/nph-Parser?Sect2=PTO1&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=1&f=G&l=50&d=PALL&RefSrch=yes&Query=PN%2F7356527].\nI am checking with the IP department at IBM about committing an implementation of the patent in Lucene, and will update here as soon as I know where it stands - could you hold committing this until then?\n",
            "date": "2010-05-23T10:30:22.229+0000",
            "id": 14
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "Thank you - yes, I think we will need to wait with this. I wasn't aware of the patent when I implemented this patch, and now after reading it I have the impression that it covers the exact algorithm described in the original paper, so perhaps we should be in the clear if we focus on other methods or a modified versions of it, but of course IANAL.",
            "date": "2010-05-24T05:10:50.375+0000",
            "id": 15
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "Doron, were you able to check on the patent situation? If there's a chance of solving this in a positive way, how long do you think this could take?",
            "date": "2010-08-10T00:11:03.945+0000",
            "id": 16
        },
        {
            "author": "Doron Cohen",
            "body": "Hi Andrzej, chances seem pretty good. We were thinking about further developing the index pruning implementation, however didn't get to it, hope to, later this year. If you rather not wait for that please go ahead with the current implementation. Thanks, Doron.",
            "date": "2010-08-10T20:25:08.172+0000",
            "id": 17
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "That's great news, thanks! However, now you got me thinking ... considering there is legal aspect to the matter, do we (the Apache Lucene project) need something more substantial from IBM (e.g. a statement from your IP dept.) than just your \"go ahead\" in a JIRA comment?",
            "date": "2010-08-10T21:08:41.487+0000",
            "id": 18
        },
        {
            "author": "Doron Cohen",
            "body": "Hi Andrzej, I would have asked the same question :) Fortunately this is cleared by the following patch I am attaching next (with that little check box in the Attach-Files dialog)",
            "date": "2010-08-12T10:12:42.129+0000",
            "id": 19
        },
        {
            "author": "Doron Cohen",
            "body": "The pruning framework is pretty cool - it is quite easy to add a new pruning policy!\n\nInitially I planned to focus on CarmelPruningPolicy plus add the more sophisticated algorithm (tpoK) described in the paper, but eventually found myself doing more changes - Andrzej, I hope you like the changes - like some methods I renamed - otherwise please feel free to rename them back.\n\n+Patch Details+\n\n* Documentation changes - mainly moved things to where I thought they belong, like moving from CarmelPruning to TFPruning the general discussion that applies to any Term pruning implementation.\n* Renamed some of TermPruningPolicy methods for better readability - well, to me :\\) -- hope you agree with the new names, othewise please feel free to change back.\n* Renamed CarmelTermPruningPolicy to CarmelTermPruningUniformPolicy. - quite a long name... but descriptive, as this an enhanced form of the \"uniform\" case from the paper. Modified documentation accordingly.\n* Added CarmelTermPruningTopKPolicy - this is the more sophisticated/strong form of pruning described in the paper. (Test case added.)\n* Fixed some compiler warnings (1.5, Lucene.Version..)\n* Bug (\\?) in CarmelTermPruningPolicy in initTermPositions() - it sorts by docid before selecting the top subset, but in fact this seems dead code? Added a \"TODO deadcode\" there, maybe I am missing something.\n* Enabled topK pruning through the PruningTool program (untested)\n* Simplified the if statements in PruningRedaer.PruningTermEnum.next() - hopefully not missing something t here...\n\nThere's more to do though not sure when I'll have the cycles..:\n* Quality/performance test for the topK pruning algorithm - using LAtimes or some other judged collection. \n  Or perhaps Robert can try it on that Persian test collection. \n* Add also the \"Delta Pruning\" policy as described in the paper\n* Junit for CarmelTermPruningUniformPolicy\n\nDoron",
            "date": "2010-08-12T12:12:09.261+0000",
            "id": 20
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "Doron, thank you very much for pushing forward this issue! I think your patch looks good, I'm still reviewing it in the light of 3.1 APIs. It's great that you added a new policy and test cases - this looks solid now.\n\nIn the meantime however I still doubt if the JIRA checkbox is a sufficient counterweight to a possibility of a patent infringement suit against users of Lucene... I think in cases like this, where there is a known existing patent that this implementation uses, the ASF requires an explicit software grant to be made (http://www.apache.org/licenses/software-grant.txt) which would protect Lucene users from infringing on IBM's IP. I'll forward this to legal@apache.org to see what they say about it - if you can obtain such a grant without too much trouble then I'm sure we could then close this issue.",
            "date": "2010-08-13T09:11:59.696+0000",
            "id": 21
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "ASF legal thinks this is sufficient, so fortunately a software grant is not needed and from a legal point of view we can commit it. Yay!",
            "date": "2010-08-13T15:29:29.962+0000",
            "id": 22
        },
        {
            "author": "Doron Cohen",
            "body": "Great!",
            "date": "2010-08-13T16:02:50.546+0000",
            "id": 23
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "bq. Renamed some of TermPruningPolicy methods for better readability - well, to me :)\n\nI agree, this looks clearer now.\n\nbq. Bug (?) in CarmelTermPruningPolicy in initTermPositions() - it sorts by docid before selecting the top subset, but in fact this seems dead code? Added a \"TODO deadcode\" there, maybe I am missing something.\n\nWell spotted, indeed this section was not needed.\n\nbq. Simplified the if statements in PruningRedaer.PruningTermEnum.next() - hopefully not missing something t here...\n\nLooks good to me. Thanks!\n\nOkay, if there are no further comments I'd like to commit this soon.",
            "date": "2010-08-23T15:20:48.887+0000",
            "id": 24
        },
        {
            "author": "Kaktu Chakarabati",
            "body": "Hey,\nWhile trying to use this (wonderful!) component I noticed few things that might require some work:\n\n1. The issue says this affects lucene 2.9 as well, however the code seems to be hard-coded for 3.0 (uses the LUCENE_30 constant, as well as some new API's such as IndexWriterConfig).\n     I created a patch that'll make it work with 2.9.3 (so I can use it with a Solr 1.4.1 deployment), and I can post it as a patch if seems useful, but I suspect we might want to come up with a more generic solution as well\n     as clear definition of supported versions. Personally I think will be very useful to have a backport for 2.9.x so that users of current stable Solr release can use it (1.4.x)\n\n2. The code does not compile with the trunk (lucene/solr 4.0). Is this known issue? something we wish to solve? \n\n3. When using it with the 3.0 branch, it does indeed work, However when it reads an older version of the index and emits a newer one (e.g reads in 2.9.x, spits out 3.x) it renders the pruned index unusable by some platforms (e.g solr 1.4.x as mentioned above). Is this something that can be fixed? i.e forcing output index to be same version as input one? I was gonna do some work on my own there but this issue seems alittle more delicate and requires deeper understanding of lucene innards than i afford..\n\n-Chak\n     ",
            "date": "2010-12-27T18:29:31.914+0000",
            "id": 25
        },
        {
            "author": "Doron Cohen",
            "body": "I plan to work on this for 3.x and trunk, did not notice that it was marked for 2.9, but can look at that as well. \nAndrej are you working on this or can I take it? \nAs for writing in 2.9 format I think it is better to backport to 2.9 rather than having the 3.x (or trunk) version write in 2.9 format?\nDoron",
            "date": "2010-12-27T23:33:22.303+0000",
            "id": 26
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "Doron, feel free to work on this - I won't be able to do any work on this in January.",
            "date": "2011-01-03T14:20:37.206+0000",
            "id": 27
        },
        {
            "author": "Robert Muir",
            "body": "bulk move 3.2 -> 3.3",
            "date": "2011-06-03T16:40:46.987+0000",
            "id": 28
        },
        {
            "author": "luo",
            "body": "where can i download the codes about the pruning ,i can't found in \nhttps://svn.apache.org/repos/asf/lucene/dev/branches/branch_3x\n\nthanks",
            "date": "2011-08-29T08:29:06.599+0000",
            "id": 29
        },
        {
            "author": "Doron Cohen",
            "body": "Updated patch for current 3x.",
            "date": "2012-01-23T15:31:36.955+0000",
            "id": 30
        },
        {
            "author": "Doron Cohen",
            "body": "Getting to this, at last. \n\nI did not handle the above TODO's and I rather commit so they can be handled later separately (\"progress not perfection\" as Mike says). \n\nChanges in this patch: \n- PruningReader overrides also getSequentialSubReaders(), otherwise no pruning takes place on sub-readers (and tests fail). \n- StorePruningPolicy fixed to use FieldInfos API.\n\nI modified for Idea and maven by following templates for other contrib components but have no way to test this and would appreciate a review of this.",
            "date": "2012-01-23T15:33:18.973+0000",
            "id": 31
        },
        {
            "author": "Doron Cohen",
            "body": "I now see that all other contrib components have svn:ignore for *.iml and pom.xml - I'll add that for pruning as well (though it is not in the attached patch).",
            "date": "2012-01-23T15:40:32.796+0000",
            "id": 32
        },
        {
            "author": "Steve Rowe",
            "body": "Hi Doron,\n\nbq. I modified for Idea and maven by following templates for other contrib components but have no way to test this and would appreciate a review of this.\n\nI looked at these configurations and they should be functional.  I didn't test them, but I will once they have been committed.",
            "date": "2012-01-23T15:42:22.948+0000",
            "id": 33
        },
        {
            "author": "Doron Cohen",
            "body": "bq. I didn't test them, but I will once they have been committed.\n\nGreat, thanks! ",
            "date": "2012-01-23T15:57:02.126+0000",
            "id": 34
        },
        {
            "author": "Doron Cohen",
            "body": "I ran 'javadocs' under 3x/lucene/contrib/pruning and 'javadocs-all' under 3x/lucene. \n\nThe latter failed due to multiple package.html under o.a.l.index - in core and under contrib/pruning. \n\nEntirely renaming the package to o.a.l.pruning.index won't work because PruningReader accesses package protected SegmentTermVector.\n\nI can move the other classes to that new package and keep only PruningReader in that \"index friend\" package. (Unless there are javadoc/ant tricks that will avoid this error and still generate valid javadocs in both cases).\n",
            "date": "2012-01-24T12:53:35.908+0000",
            "id": 35
        },
        {
            "author": "Doron Cohen",
            "body": "Updated patch: package.html and all pruning classes moved to another package, except for PruningReader. Now ant javadocs-all passes as well. There are 3 TODO's:\n# implement CarmelTermPruningDeltaTopPolicy\n# dead code question in CarmelUniformTermPruningPolicy\n# missing details in package.html\n\nThe first one can wait but the other two I would like to handle before committing.",
            "date": "2012-01-30T18:30:43.160+0000",
            "id": 36
        },
        {
            "author": "Doron Cohen",
            "body": "That dead code was removed and some javadocs added. \nStill room for more javadocs - e.g.  the static tool - and better test coverage.\nCommitted to 3x: r1237937.",
            "date": "2012-01-30T19:20:22.968+0000",
            "id": 37
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "Excellent, thanks for seeing this through!",
            "date": "2012-01-30T20:16:11.289+0000",
            "id": 38
        },
        {
            "author": "Doron Cohen",
            "body": "bq. Excellent, thanks for seeing this through!\n\nYeah, only more than a year delay ;)\n\nBTW in trunk it will be under modules.",
            "date": "2012-01-30T21:23:40.199+0000",
            "id": 39
        },
        {
            "author": "Doron Cohen",
            "body": "while merging to trunk I noticed that idea's settings for modules/queries and modules/queryparser refer to lucene/contrib instead of modules. Seems trivial to fix but I have no Idea installed at the moment so no way to verify. Created LUCENE-3737 to handle that later.",
            "date": "2012-01-31T11:42:51.951+0000",
            "id": 40
        },
        {
            "author": "Robert Muir",
            "body": "This is really still open I think for the 4.x port.\n\nTo eliminate confusion: I'll mark this resolved and create a 4.0 issue to port pruning to trunk APIs.",
            "date": "2012-03-25T16:49:27.576+0000",
            "id": 41
        },
        {
            "author": "Robert Muir",
            "body": "Marking this resolved: I created LUCENE-3917 for the 4.x port for JIRA organization purposes.",
            "date": "2012-03-25T16:51:31.734+0000",
            "id": 42
        }
    ],
    "component": "modules/other",
    "description": "This module provides tools to produce a subset of input indexes by removing postings data for those terms where their in-document frequency is below a specified threshold. The net effect of this processing is a much smaller index that for common types of queries returns nearly identical top-N results as compared with the original index, but with increased performance. \n\nOptionally, stored values and term vectors can also be removed. This functionality is largely independent, so it can be used without term pruning (when term freq. threshold is set to 1).\n\nAs the threshold value increases, the total size of the index decreases, search performance increases, and recall decreases (i.e. search quality deteriorates). NOTE: especially phrase recall deteriorates significantly at higher threshold values. \n\nPrimary purpose of this class is to produce small first-tier indexes that fit completely in RAM, and store these indexes using IndexWriter.addIndexes(IndexReader[]). Usually the performance of this class will not be sufficient to use the resulting index view for on-the-fly pruning and searching. \n\nNOTE: If the input index is optimized (i.e. doesn't contain deletions) then the index produced via IndexWriter.addIndexes(IndexReader[]) will preserve internal document id-s so that they are in sync with the original index. This means that all other auxiliary information not necessary for first-tier processing, such as some stored fields, can also be removed, to be quickly retrieved on-demand from the original index using the same internal document id. \n\nThreshold values can be specified globally (for terms in all fields) using defaultThreshold parameter, and can be overriden using per-field or per-term values supplied in a thresholds map. Keys in this map are either field names, or terms in field:text format. The precedence of these values is the following: first a per-term threshold is used if present, then per-field threshold if present, and finally the default threshold.\n\nA command-line tool (PruningTool) is provided for convenience. At this moment it doesn't support all functionality available through API.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1812",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Static index pruning by in-document term frequency (Carmel pruning)",
    "systemSpecification": null,
    "version": ""
}